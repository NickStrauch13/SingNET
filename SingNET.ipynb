{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e1d266c-7afa-43a4-a521-2aca2fa1bdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646e448b-d7ff-4e85-9c7b-7392e1eb4c28",
   "metadata": {},
   "source": [
    "##### Input size = [3, 385, 1085]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8cba515-561c-40f9-a519-103e3586ab4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def swish(x):\n",
    "    return x/(1+torch.exp(-x))\n",
    "\n",
    "# Custom 2d average pooling. Used directly prior to the fully connected layer(s)\n",
    "#\n",
    "# dims: Tuple representing the two dimensions to pool.\n",
    "# keep_dims: Boolean determining if output should preserve number of input dimensions.\n",
    "def avg_pool(x, dims, keep_dims=False):\n",
    "    summed_tensor = torch.sum(x, dim=dims)\n",
    "    if keep_dims:\n",
    "        reshaped_tensor = summed_tensor.unsqueeze(dim=dims[0]).unsqueeze(dim=dims[1])\n",
    "        return reshaped_tensor\n",
    "    return summed_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94a37aad-a821-40a8-b88f-1e12b31f0aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MobileNET arch\n",
    "class SingNET(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SingNET, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(32, 32, kernel_size=1, stride=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=1, stride=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(64, 64, kernel_size=1, stride=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(64, 64, kernel_size=1, stride=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=1, stride=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.layer6 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=2),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(128, 128, kernel_size=1, stride=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.layer7_12 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(128, 128, kernel_size=1, stride=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.layer13 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=2),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(128, 256, kernel_size=1, stride=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.layer14 = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=2),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(256, 512, kernel_size=1, stride=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(512, 5)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.bn1(self.conv1(x))\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.layer5(out)\n",
    "        out = self.layer6(out)\n",
    "        out = self.layer7_12(out)\n",
    "        out = self.layer7_12(out)\n",
    "        out = self.layer7_12(out)\n",
    "        out = self.layer7_12(out)\n",
    "        out = self.layer7_12(out)\n",
    "        out = self.layer7_12(out)\n",
    "        out = self.layer13(out)\n",
    "        out = self.layer14(out)\n",
    "        \n",
    "        out = avg_pool(out, (2,3))\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c854ab3-fb1a-4a79-b303-e6a8bc85c149",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(\"data/costa_rica/spectrogram/Broad-billed_Motmot/Broad-billed_Motmot_105_0.jpg\")\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "img_tensor = transform(img).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c2d8d2b-517f-4f14-9d49-913562f8bf36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 385, 1085])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_tensor.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27251214-a651-4f54-82c6-1610e2d89a80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.9098, -0.4664, -0.2132, -3.4092,  3.8125]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = SingNET()\n",
    "\n",
    "net.forward(img_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6446a09d-64bb-4949-a79d-852263086adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1387589\n"
     ]
    }
   ],
   "source": [
    "num_params = sum(p.numel() for p in net.parameters())\n",
    "print(num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b23af5-e606-4284-bb0d-931478107b70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19b4ba99-0b04-4e1e-a26e-d1b6ef951e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, epochs=100, batch_size=8, lr=0.005, L2_reg=0.01, saved_filepath=\"saved_models/test_model.pt\"):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"Device in use: {device}\")\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "        #torch.cuda.max_memory_allocated(max_split_size_mb=1024)\n",
    "    net.to(device)\n",
    "    print(\"==> Starting Data Preparation...\")\n",
    "    data_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                        std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    train_set = ImageFolder(\"data/costa_rica/train\", transform=data_transform)\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "    val_set = ImageFolder(\"data/costa_rica/validation\", transform=data_transform)\n",
    "    val_loader = DataLoader(val_set, batch_size=1, shuffle=False, num_workers=1)\n",
    "    \n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=L2_reg)\n",
    "    #scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[int(epochs*0.5), int(epochs*0.75)], gamma=0.1)\n",
    "    best_acc = 0\n",
    "    \n",
    "    print(\"==> Training Initiated...\")\n",
    "    for epoch in range(0, epochs):\n",
    "        print(f\"Epoch {epoch}/{epochs}\")\n",
    "        net.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net.forward(inputs)\n",
    "            loss = loss_function(outputs, targets)\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "        print(f\"Train Loss: {round(train_loss,3)} | Train Acc: {round(correct/total,3)}\")\n",
    "        #scheduler.step()\n",
    "        \n",
    "        net.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = net.forward(inputs)\n",
    "                loss = loss_function(outputs, targets)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "        val_acc = correct/total\n",
    "        \n",
    "        print(f\"Val Loss:   {round(val_loss,3)} | Val Acc:   {round(val_acc,3)}\")\n",
    "        \n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            print(\"Saving Model...\")\n",
    "            torch.save(net.state_dict(), saved_filepath)\n",
    "    \n",
    "    print(\"=========> Training Complete <=========\")\n",
    "    print(f\"Best Validation Accuracy: {best_acc}\")\n",
    "            \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "152a3fb0-0f2d-45ab-bc03-b4097bff2f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device in use: cuda\n",
      "==> Starting Data Preparation...\n",
      "==> Training Initiated...\n",
      "Epoch 0/100\n",
      "Train Loss: 445.40128153562546 | Train Acc: 0.3524416135881104\n",
      "Val Loss:   715.9112917482853 | Val Acc:   0.375\n",
      "Saving Model...\n",
      "Epoch 1/100\n",
      "Train Loss: 296.87821167707443 | Train Acc: 0.3913658881811748\n",
      "Val Loss:   216.21432462288067 | Val Acc:   0.4375\n",
      "Saving Model...\n",
      "Epoch 2/100\n",
      "Train Loss: 243.63993626832962 | Train Acc: 0.44019815994338285\n",
      "Val Loss:   220.17544424533844 | Val Acc:   0.4375\n",
      "Epoch 3/100\n",
      "Train Loss: 187.86348417401314 | Train Acc: 0.5746638358103326\n",
      "Val Loss:   291.85523676872253 | Val Acc:   0.1625\n",
      "Epoch 4/100\n",
      "Train Loss: 162.0486858189106 | Train Acc: 0.6178343949044586\n",
      "Val Loss:   264.0709728002548 | Val Acc:   0.4375\n",
      "Epoch 5/100\n",
      "Train Loss: 152.70200729370117 | Train Acc: 0.640481245576787\n",
      "Val Loss:   290.76832419633865 | Val Acc:   0.4375\n",
      "Epoch 6/100\n",
      "Train Loss: 160.31395837664604 | Train Acc: 0.6242038216560509\n",
      "Val Loss:   264.84085220098495 | Val Acc:   0.4375\n",
      "Epoch 7/100\n",
      "Train Loss: 148.7601079940796 | Train Acc: 0.6808209483368719\n",
      "Val Loss:   273.11311703920364 | Val Acc:   0.44375\n",
      "Saving Model...\n",
      "Epoch 8/100\n",
      "Train Loss: 151.53042736649513 | Train Acc: 0.6871903750884643\n",
      "Val Loss:   219.7279577255249 | Val Acc:   0.4375\n",
      "Epoch 9/100\n",
      "Train Loss: 132.2474092245102 | Train Acc: 0.7261146496815286\n",
      "Val Loss:   164.69207463786006 | Val Acc:   0.56875\n",
      "Saving Model...\n",
      "Epoch 10/100\n",
      "Train Loss: 136.89244803786278 | Train Acc: 0.7204529370134466\n",
      "Val Loss:   210.03544828295708 | Val Acc:   0.4625\n",
      "Epoch 11/100\n",
      "Train Loss: 123.01752419769764 | Train Acc: 0.7565463552724699\n",
      "Val Loss:   160.5110845565796 | Val Acc:   0.54375\n",
      "Epoch 12/100\n",
      "Train Loss: 120.6459613442421 | Train Acc: 0.7650389242745931\n",
      "Val Loss:   195.84000482410192 | Val Acc:   0.48125\n",
      "Epoch 13/100\n",
      "Train Loss: 122.6537870913744 | Train Acc: 0.7643312101910829\n",
      "Val Loss:   165.05195198394358 | Val Acc:   0.54375\n",
      "Epoch 14/100\n",
      "Train Loss: 110.20492872595787 | Train Acc: 0.7848549186128804\n",
      "Val Loss:   199.220704510808 | Val Acc:   0.4625\n",
      "Epoch 15/100\n",
      "Train Loss: 102.47637265175581 | Train Acc: 0.8011323425336164\n",
      "Val Loss:   131.98303484916687 | Val Acc:   0.70625\n",
      "Saving Model...\n",
      "Epoch 16/100\n",
      "Train Loss: 99.40942323952913 | Train Acc: 0.8067940552016986\n",
      "Val Loss:   148.49979884875938 | Val Acc:   0.65625\n",
      "Epoch 17/100\n",
      "Train Loss: 104.1519525051117 | Train Acc: 0.7983014861995754\n",
      "Val Loss:   202.8088831976056 | Val Acc:   0.46875\n",
      "Epoch 18/100\n",
      "Train Loss: 102.12151065468788 | Train Acc: 0.7983014861995754\n",
      "Val Loss:   263.0234845280647 | Val Acc:   0.4375\n",
      "Epoch 19/100\n",
      "Train Loss: 99.4333557933569 | Train Acc: 0.8004246284501062\n",
      "Val Loss:   268.02233079075813 | Val Acc:   0.4375\n",
      "Epoch 20/100\n",
      "Train Loss: 97.51096018403769 | Train Acc: 0.8046709129511678\n",
      "Val Loss:   258.7651515305042 | Val Acc:   0.4375\n",
      "Epoch 21/100\n",
      "Train Loss: 85.5134294629097 | Train Acc: 0.8266100495399858\n",
      "Val Loss:   236.76412534713745 | Val Acc:   0.4625\n",
      "Epoch 22/100\n",
      "Train Loss: 90.4096973836422 | Train Acc: 0.8237791932059448\n",
      "Val Loss:   263.34271866083145 | Val Acc:   0.4375\n",
      "Epoch 23/100\n",
      "Train Loss: 87.86809708923101 | Train Acc: 0.8294409058740269\n",
      "Val Loss:   289.25094482302666 | Val Acc:   0.4375\n",
      "Epoch 24/100\n",
      "Train Loss: 85.12107560038567 | Train Acc: 0.8287331917905166\n",
      "Val Loss:   269.47130116820335 | Val Acc:   0.4375\n",
      "Epoch 25/100\n",
      "Train Loss: 84.51498233526945 | Train Acc: 0.832271762208068\n",
      "Val Loss:   262.9979475736618 | Val Acc:   0.4375\n",
      "Epoch 26/100\n",
      "Train Loss: 83.71002627909184 | Train Acc: 0.83793347487615\n",
      "Val Loss:   259.2191389799118 | Val Acc:   0.4375\n",
      "Epoch 27/100\n",
      "Train Loss: 91.75113267824054 | Train Acc: 0.8301486199575372\n",
      "Val Loss:   262.29667365550995 | Val Acc:   0.4375\n",
      "Epoch 28/100\n",
      "Train Loss: 78.73009326308966 | Train Acc: 0.8492569002123143\n",
      "Val Loss:   277.8047041296959 | Val Acc:   0.4375\n",
      "Epoch 29/100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m net \u001b[38;5;241m=\u001b[39m SingNET()\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(net, epochs, batch_size, lr, L2_reg, saved_filepath)\u001b[0m\n\u001b[0;32m     37\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     39\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 40\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     42\u001b[0m total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "net = SingNET()\n",
    "train(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b5957f-a5ab-4a47-8e7f-fd71dbfa729e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
